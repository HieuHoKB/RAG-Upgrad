{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lf5lYawIw8tE",
   "metadata": {
    "id": "lf5lYawIw8tE"
   },
   "source": [
    "# **Extracting Information from Legal Documents Using RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "FsI0LEQyh1ol",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30679,
     "status": "ok",
     "timestamp": 1748873701772,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "FsI0LEQyh1ol",
    "outputId": "c87f3994-fcd7-4f14-f7b5-22fbb2b31704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NY1InIbkw80B",
   "metadata": {
    "id": "NY1InIbkw80B"
   },
   "source": [
    "## **Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3403a4b5",
   "metadata": {
    "id": "3403a4b5"
   },
   "source": [
    "The main objective of this assignment is to process and analyse a collection text files containing legal agreements (e.g., NDAs) to prepare them for implementing a **Retrieval-Augmented Generation (RAG)** system. This involves:\n",
    "\n",
    "* Understand the Cleaned Data : Gain a comprehensive understanding of the structure, content, and context of the cleaned dataset.\n",
    "* Perform Exploratory Analysis : Conduct bivariate and multivariate analyses to uncover relationships and trends within the cleaned data.\n",
    "* Create Visualisations : Develop meaningful visualisations to support the analysis and make findings interpretable.\n",
    "* Derive Insights and Conclusions : Extract valuable insights from the cleaned data and provide clear, actionable conclusions.\n",
    "* Document the Process : Provide a detailed description of the data, its attributes, and the steps taken during the analysis for reproducibility and clarity.\n",
    "\n",
    "The ultimate goal is to transform the raw text data into a clean, structured, and analysable format that can be effectively used to build and train a RAG system for tasks like information retrieval, question-answering, and knowledge extraction related to legal agreements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3TTEcbb5hIM-",
   "metadata": {
    "id": "3TTEcbb5hIM-"
   },
   "source": [
    "### **Business Value**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZsfkEL2CgljF",
   "metadata": {
    "id": "ZsfkEL2CgljF"
   },
   "source": [
    "The project aims to leverage RAG to enhance legal document processing for businesses, law firms, and regulatory bodies. The key business objectives include:\n",
    "\n",
    "* Faster Legal Research: <br> Reduce the time lawyers and compliance officers spend searching for relevant case laws, precedents, statutes, or contract clauses.\n",
    "* Improved Contract Analysis: <br> Automatically extract key terms, obligations, and risks from lengthy contracts.\n",
    "* Regulatory Compliance Monitoring: <br> Help businesses stay updated with legal and regulatory changes by retrieving relevant legal updates.\n",
    "* Enhanced Decision-Making: <br> Provide accurate and context-aware legal insights to assist in risk assessment and legal strategy.\n",
    "\n",
    "\n",
    "**Use Cases**\n",
    "* Legal Chatbots\n",
    "* Contract Review Automation\n",
    "* Tracking Regulatory Changes and Compliance Monitoring\n",
    "* Case Law Analysis of past judgments\n",
    "* Due Diligence & Risk Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rDp_EWxVOhUu",
   "metadata": {
    "id": "rDp_EWxVOhUu"
   },
   "source": [
    "## **1. Data Loading, Preparation and Analysis** <font color=red> [20 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JZGTCfyUxalZ",
   "metadata": {
    "id": "JZGTCfyUxalZ"
   },
   "source": [
    "### **1.1 Data Understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ok6sSYNAiG8V",
   "metadata": {
    "id": "ok6sSYNAiG8V"
   },
   "source": [
    "The dataset contains legal documents and contracts collected from various sources. The documents are present as text files (`.txt`) in the *corpus* folder.\n",
    "\n",
    "There are four types of documents in the *courpus* folder, divided into four subfolders.\n",
    "- `contractnli`: contains various non-disclosure and confidentiality agreements\n",
    "- `cuad`: contains contracts with annotated legal clauses\n",
    "- `maud`: contains various merger/acquisition contracts and agreements\n",
    "- `privacy_qa`: a question-answering dataset containing privacy policies\n",
    "\n",
    "The dataset also contains evaluation files in JSON format in the *benchmark* folder. The files contain the questions and their answers, along with sources. For each of the above four folders, there is a `json` file: `contractnli.json`, `cuad.json`, `maud.json` `privacy_qa.json`. The file structure is as follows:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"tests\": [\n",
    "        {\n",
    "            \"query\": <question1>,\n",
    "            \"snippets\": [{\n",
    "                    \"file_path\": <source_file1>,\n",
    "                    \"span\": [ begin_position, end_position ],\n",
    "                    \"answer\": <relevant answer to the question 1>\n",
    "                },\n",
    "                {\n",
    "                    \"file_path\": <source_file2>,\n",
    "                    \"span\": [ begin_position, end_position ],\n",
    "                    \"answer\": <relevant answer to the question 2>\n",
    "                }, ....\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"query\": <question2>,\n",
    "            \"snippets\": [{<answer context for que 2>}]\n",
    "        },\n",
    "        ... <more queries>\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S7Ac8VxvjWnw",
   "metadata": {
    "id": "S7Ac8VxvjWnw"
   },
   "source": [
    "### **1.2 Load and Preprocess the data** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gJ8fA4Nh3fHg",
   "metadata": {
    "id": "gJ8fA4Nh3fHg"
   },
   "source": [
    "#### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "BqyFHhSn48tC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57545,
     "status": "ok",
     "timestamp": 1748873782814,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "BqyFHhSn48tC",
    "outputId": "da420fff-1022-4a2b-f303-3a283d684a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.5/438.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.7/96.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# The following libraries might be useful\n",
    "!pip install -q langchain-openai\n",
    "!pip install -U -q langchain-community\n",
    "!pip install -U -q langchain-chroma\n",
    "!pip install -U -q datasets\n",
    "!pip install -U -q ragas\n",
    "!pip install -U -q rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zOMf-tfIiOlp",
   "metadata": {
    "id": "zOMf-tfIiOlp"
   },
   "source": [
    "#### **1.2.1** <font color=red> [3 marks] </font>\n",
    "Load all `.txt` files from the folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea36ba",
   "metadata": {
    "id": "f2ea36ba"
   },
   "source": [
    "You can utilise document loaders from the options provided by the LangChain community.\n",
    "\n",
    "Optionally, you can also read the files manually, while ensuring proper handling of encoding issues (e.g., utf-8, latin1). In such case, also store the file content along with metadata (e.g., file name, directory path) for traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "I9rTY8DWx2Wj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6095,
     "status": "ok",
     "timestamp": 1748879592611,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "I9rTY8DWx2Wj",
    "outputId": "556a024e-1603-4248-dd39-993eaa893df8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 95 documents from contractnli\n",
      "Loaded 462 documents from cuad\n",
      "Loaded 134 documents from maud\n",
      "Loaded 7 documents from privacy_qa\n",
      "Created file content map with 698 entries.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "# Define the base path for the corpus\n",
    "base_path = \"/content/drive/MyDrive/Colab Notebooks/RAG Upgrad/rag_legal/corpus\"\n",
    "# Dictionary to store the loaded documents\n",
    "documents = {}\n",
    "\n",
    "file_content_map = {}\n",
    "\n",
    "\n",
    "# Iterate through the subfolders\n",
    "for folder in [\"contractnli\", \"cuad\", \"maud\", \"privacy_qa\"]:\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    documents[folder] = []\n",
    "\n",
    "    # Load all .txt files in the folder using TextLoader\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            try:\n",
    "                loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "                docs = loader.load()\n",
    "                for doc in docs:\n",
    "                    documents[folder].append(doc)\n",
    "\n",
    "                file_content_map[file_path] = doc.page_content # Store original content before preprocessing\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {file_path}: {e}\")\n",
    "\n",
    "for folder, docs in documents.items():\n",
    "    print(f\"Loaded {len(docs)} documents from {folder}\")\n",
    "print(f\"Created file content map with {len(file_content_map)} entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00906bb6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1748873899575,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "00906bb6",
    "outputId": "7a16ee64-0111-4590-e1a2-d40f3de3c2e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in contractnli: 95\n",
      "Total number of documents: 95\n",
      "Number of documents in cuad: 462\n",
      "Total number of documents: 557\n",
      "Number of documents in maud: 134\n",
      "Total number of documents: 691\n",
      "Number of documents in privacy_qa: 7\n",
      "Total number of documents: 698\n"
     ]
    }
   ],
   "source": [
    "total_docs =[]\n",
    "for folder, docs in documents.items():\n",
    "    no_of_docs = []\n",
    "    for doc in docs:\n",
    "        no_of_docs.append(doc.page_content)\n",
    "    total_docs.extend(no_of_docs)\n",
    "    print(f\"Number of documents in {folder}: {len(no_of_docs)}\")\n",
    "    print(f\"Total number of documents: {len(total_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb384152",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1748873903545,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "bb384152",
    "outputId": "bc7b28a7-1551-4eaf-befc-2801c6546ee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(docs[0]))  # Check the type of the first element in docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df394c73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1748873904471,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "df394c73",
    "outputId": "32e82dc8-9984-4e10-934b-eaed9ab68383"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '/content/drive/MyDrive/Colab Notebooks/RAG Upgrad/rag_legal/corpus/privacy_qa/Viber Messenger.txt'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata  # Access the metadata of the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K4HYLoUjwmMs",
   "metadata": {
    "id": "K4HYLoUjwmMs"
   },
   "source": [
    "#### **1.2.2** <font color=red> [2 marks] </font>\n",
    "Preprocess the text data to remove noise and prepare it for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9793fdf",
   "metadata": {
    "id": "e9793fdf"
   },
   "source": [
    "Remove special characters, extra whitespace, and irrelevant content such as email and telephone contact info.\n",
    "Normalise text (e.g., convert to lowercase, remove stop words).\n",
    "Handle missing or corrupted data by logging errors and skipping problematic files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53d44537",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15268,
     "status": "ok",
     "timestamp": 1748873924338,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "53d44537",
    "outputId": "1ac4c219-8168-4dd4-867b-6b73ff4719c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# Clean and preprocess the data\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by removing special characters, extra whitespace,\n",
    "    and irrelevant content such as email and telephone contact info.\n",
    "    Normalize text by converting to lowercase and removing stop words.\n",
    "    \"\"\"\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Remove phone numbers\n",
    "    text = re.sub(r'\\b\\d{10,}\\b', '', text)\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "for folder, docs in documents.items():\n",
    "    for doc in docs:\n",
    "        doc.page_content = preprocess_text(doc.page_content)\n",
    "\n",
    "print(\"Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e90470",
   "metadata": {
    "id": "b9e90470"
   },
   "source": [
    "### **1.3 Exploratory Data Analysis** <font color=red> [10 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Nd1K4yhIzyPp",
   "metadata": {
    "id": "Nd1K4yhIzyPp"
   },
   "source": [
    "#### **1.3.1** <font color=red> [1 marks] </font>\n",
    "Calculate the average, maximum and minimum document length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f66df0ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1748873927404,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "f66df0ac",
    "outputId": "c646ec23-0176-4c90-8cb8-4986ebd01665"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for folder, docs in documents.items():\n",
    "    for doc in docs:\n",
    "        lengths.append(len(doc.page_content))\n",
    "print(len(lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tQT1UIcOHSp9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1748873930541,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "tQT1UIcOHSp9",
    "outputId": "9310710d-d35c-4a2c-8744-7fc4aaec0ab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Document Length: 99063.25787965616\n",
      "Maximum Document Length: 957212\n",
      "Minimum Document Length: 1329\n",
      "Total Number of Documents: 698\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average, maximum, and minimum document length\n",
    "def calculate_document_lengths(documents):\n",
    "    lengths = []\n",
    "    for folder, docs in documents.items():\n",
    "        for doc in docs:\n",
    "            lengths.append(len(doc.page_content))  # Access \"page_content\" as a dictionary key\n",
    "\n",
    "    if lengths:\n",
    "        avg_length = sum(lengths) / len(lengths)\n",
    "        max_length = max(lengths)\n",
    "        min_length = min(lengths)\n",
    "        total_documents = len(lengths)\n",
    "        return avg_length, max_length, min_length, total_documents\n",
    "    else:\n",
    "        return 0, 0, 0\n",
    "\n",
    "# Example usage\n",
    "avg_length, max_length, min_length, total_documents = calculate_document_lengths(documents)\n",
    "print(f\"Average Document Length: {avg_length}\")\n",
    "print(f\"Maximum Document Length: {max_length}\")\n",
    "print(f\"Minimum Document Length: {min_length}\")\n",
    "print(f\"Total Number of Documents: {total_documents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18xQu__O0wLv",
   "metadata": {
    "id": "18xQu__O0wLv"
   },
   "source": [
    "#### **1.3.2** <font color=red> [4 marks] </font>\n",
    "Analyse the frequency of occurrence of words and find the most and least occurring words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IQ_i5YfFH2dg",
   "metadata": {
    "id": "IQ_i5YfFH2dg"
   },
   "source": [
    "Find the 20 most common and least common words in the text. Ignore stop words such as articles and prepositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "Q8eiDTy2Ic8z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53594,
     "status": "ok",
     "timestamp": 1748873991605,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "Q8eiDTy2Ic8z",
    "outputId": "38b32567-f02a-45f5-d717-ae56ad9e5a70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Most Common Words: [('company', 148167), ('shall', 107995), ('agreement', 104559), ('section', 75344), ('parent', 58009), ('party', 49656), ('date', 39281), ('time', 35251), ('material', 34208), ('merger', 33843), ('subsidiaries', 33317), ('applicable', 31368), ('including', 29398), ('respect', 28848), ('may', 28069), ('stock', 26651), ('information', 25681), ('parties', 24609), ('b', 23935), ('business', 23497)]\n",
      "20 Least Common Words: [('newer', 1), ('2257522579', 1), ('peoplefuncom', 1), ('nonmarketing', 1), ('httpwwwyouronlinechoiceseu', 1), ('httpwwwaboutadsinfochoices', 1), ('checkins', 1), ('httpsvunglecomprivacypolicy', 1), ('vungle', 1), ('httpswwwtapresearchcomuserprivacy', 1), ('tapresearch', 1), ('httpsdevtapjoycomfaqtapjoyprivacypolicy', 1), ('tapjoy', 1), ('httpswwwstartappcompolicyprivacypolicy', 1), ('startappcom', 1), ('httpaboutsoomlaenduserprivacypolicy', 1), ('soomla', 1), ('httpswwwsmaatocomprivacy', 1), ('smaato', 1), ('httpspinsightmediacomprivacy', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Find frequency of occurence of words\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a function to find the most and least common words\n",
    "def find_common_words(documents, num_words=20):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_counts = Counter()\n",
    "\n",
    "    # Tokenize and count words in all documents\n",
    "    for folder, docs in documents.items():\n",
    "        for doc in docs:\n",
    "            words = word_tokenize(doc.page_content)\n",
    "            filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "            word_counts.update(filtered_words)\n",
    "\n",
    "    # Get the most and least common words\n",
    "    most_common = word_counts.most_common(num_words)\n",
    "    least_common = word_counts.most_common()[:-num_words-1:-1]\n",
    "\n",
    "    return most_common, least_common\n",
    "\n",
    "most_common, least_common = find_common_words(documents)\n",
    "print(\"20 Most Common Words:\", most_common)\n",
    "print(\"20 Least Common Words:\", least_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xlF55RNjz9pQ",
   "metadata": {
    "id": "xlF55RNjz9pQ"
   },
   "source": [
    "#### **1.3.3** <font color=red> [4 marks] </font>\n",
    "Analyse the similarity of different documents to each other based on TF-IDF vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jciCNMelOGPJ",
   "metadata": {
    "id": "jciCNMelOGPJ"
   },
   "source": [
    "Transform some documents to TF-IDF vectors and calculate their similarity matrix using a suitable distance function. If contracts contain duplicate or highly similar clauses, similarity calculation can help detect them.\n",
    "\n",
    "Identify for the first 10 documents and then for 10 random documents. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "M-_SrvDcMnKi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7895,
     "status": "ok",
     "timestamp": 1748874188142,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "M-_SrvDcMnKi",
    "outputId": "adcc26a4-1932-4076-dfb0-0e49867765fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (698, 53814)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Function to transform documents into TF-IDF vectors\n",
    "def transform_to_tfidf(documents):\n",
    "    # Combine all document contents into a list\n",
    "    all_docs = []\n",
    "    for folder, docs in documents.items():\n",
    "        for doc in docs:\n",
    "            all_docs.append(doc.page_content)\n",
    "\n",
    "    # Initialize the TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_docs)\n",
    "\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "tfidf_matrix, vectorizer = transform_to_tfidf(documents)\n",
    "\n",
    "# Print the shape of the TF-IDF matrix\n",
    "print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f73600f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1748874254582,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "f73600f2",
    "outputId": "c7ae7164-9cfe-42f1-d67f-024f6b4cc929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrix (First 10 Documents):\n",
      "[[1.         0.8270636  0.92523176 0.89357084 0.84617424 0.90547066\n",
      "  0.90187807 0.55784613 0.72899334 0.8547223 ]\n",
      " [0.8270636  1.         0.8191439  0.82327974 0.77032254 0.84548096\n",
      "  0.83705434 0.5167663  0.68886273 0.74162957]\n",
      " [0.92523176 0.8191439  1.         0.90558092 0.84680564 0.90865286\n",
      "  0.89467926 0.53854333 0.6959245  0.81215928]\n",
      " [0.89357084 0.82327974 0.90558092 1.         0.91842448 0.90333652\n",
      "  0.90034752 0.55429589 0.72401837 0.83299374]\n",
      " [0.84617424 0.77032254 0.84680564 0.91842448 1.         0.85021242\n",
      "  0.8613531  0.5438803  0.68932421 0.80423435]\n",
      " [0.90547066 0.84548096 0.90865286 0.90333652 0.85021242 1.\n",
      "  0.90592347 0.56004787 0.72211617 0.83141618]\n",
      " [0.90187807 0.83705434 0.89467926 0.90034752 0.8613531  0.90592347\n",
      "  1.         0.56730904 0.72792242 0.82849781]\n",
      " [0.55784613 0.5167663  0.53854333 0.55429589 0.5438803  0.56004787\n",
      "  0.56730904 1.         0.48179914 0.54192128]\n",
      " [0.72899334 0.68886273 0.6959245  0.72401837 0.68932421 0.72211617\n",
      "  0.72792242 0.48179914 1.         0.68418443]\n",
      " [0.8547223  0.74162957 0.81215928 0.83299374 0.80423435 0.83141618\n",
      "  0.82849781 0.54192128 0.68418443 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity scores for 10 first documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "all_docs = []  # Initialize an empty list to store all document contents\n",
    "for folder, docs in documents.items():\n",
    "    for doc in docs:\n",
    "        all_docs.append(doc.page_content)    # Select the first 10 documents\n",
    "first_10_docs = all_docs[:10]\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(first_10_docs)\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# Print the similarity matrix for the first 10 documents\n",
    "print(\"Similarity Matrix (First 10 Documents):\")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7da0ee4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1748874257005,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "e7da0ee4",
    "outputId": "89c15d54-0a7f-4187-e2c5-7ec872c566b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Similarity: 0.7903879105094995\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the average value of the similarity matrix\n",
    "average_similarity = np.mean(similarity_matrix)\n",
    "print(f\"Average Similarity: {average_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13fd3121",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1748874258973,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "13fd3121",
    "outputId": "43944241-d8e6-47f9-d600-753e2d636044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrix (First 10 Documents):\n",
      "[[1.         0.73716557 0.70906772 0.44288951 0.56858931 0.79008351\n",
      "  0.6827307  0.60140366 0.51747787 0.64913103]\n",
      " [0.73716557 1.         0.87871694 0.53362492 0.67908797 0.91377913\n",
      "  0.78240003 0.68252404 0.65375891 0.78244115]\n",
      " [0.70906772 0.87871694 1.         0.52942467 0.65492756 0.87967282\n",
      "  0.76326232 0.68360024 0.64664033 0.76726722]\n",
      " [0.44288951 0.53362492 0.52942467 1.         0.41937679 0.53023243\n",
      "  0.47811871 0.4380021  0.40745518 0.47944881]\n",
      " [0.56858931 0.67908797 0.65492756 0.41937679 1.         0.67194503\n",
      "  0.61024109 0.56514784 0.50239364 0.61441499]\n",
      " [0.79008351 0.91377913 0.87967282 0.53023243 0.67194503 1.\n",
      "  0.79983779 0.69811158 0.64295763 0.77024811]\n",
      " [0.6827307  0.78240003 0.76326232 0.47811871 0.61024109 0.79983779\n",
      "  1.         0.64889433 0.55504269 0.70654594]\n",
      " [0.60140366 0.68252404 0.68360024 0.4380021  0.56514784 0.69811158\n",
      "  0.64889433 1.         0.50997213 0.63170399]\n",
      " [0.51747787 0.65375891 0.64664033 0.40745518 0.50239364 0.64295763\n",
      "  0.55504269 0.50997213 1.         0.56270608]\n",
      " [0.64913103 0.78244115 0.76726722 0.47944881 0.61441499 0.77024811\n",
      "  0.70654594 0.63170399 0.56270608 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity scores for 10 first documents\n",
    "import random\n",
    "\n",
    "all_docs = []  # Initialize an empty list to store all document contents\n",
    "for folder, docs in documents.items():\n",
    "    for doc in docs:\n",
    "        all_docs.append(doc.page_content)    # Select the first 10 documents\n",
    "random_indices = random.sample(range(len(all_docs)), 10)\n",
    "random_docs = [all_docs[i] for i in random_indices]\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(random_docs)\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# Print the similarity matrix for the first 10 documents\n",
    "print(\"Similarity Matrix (First 10 Documents):\")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8685b45c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1748874261762,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "8685b45c",
    "outputId": "73ef0db1-b0db-45fb-b8f3-aa1e73c6f4e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Similarity: 0.6754492807353668\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the average value of the similarity matrix\n",
    "average_similarity = np.mean(similarity_matrix)\n",
    "print(f\"Average Similarity: {average_similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbfe6c1",
   "metadata": {
    "id": "dcbfe6c1"
   },
   "source": [
    "### Observation: It seems that documents that are far from each other has lower similarity score.\n",
    "### The first 10 documents has averagely higher similarity score due to their nearer positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd0f53",
   "metadata": {
    "id": "3cfd0f53"
   },
   "source": [
    "### **1.4 Document Creation and Chunking** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pCw3NzcE3waS",
   "metadata": {
    "id": "pCw3NzcE3waS"
   },
   "source": [
    "#### **1.4.1** <font color=red> [5 marks] </font>\n",
    "Perform appropriate steps to split the text into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "TjZ6yf9r2p1F",
   "metadata": {
    "executionInfo": {
     "elapsed": 110,
     "status": "ok",
     "timestamp": 1748874265696,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "TjZ6yf9r2p1F"
   },
   "outputs": [],
   "source": [
    "def generate_chunks(documents, chunk_size=200, chunk_overlap=50):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.schema import Document  # Import the Document class\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        add_start_index=True,  # Enables start_index in the output\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    for folder, docs in documents.items():\n",
    "        for doc in docs:\n",
    "            # Split the document into chunks (list of strings)\n",
    "            doc_chunks = text_splitter.split_text(doc.page_content)\n",
    "            for chunk in doc_chunks:\n",
    "                # Create Document objects for each chunk and associate metadata\n",
    "                chunks.append(Document(page_content=chunk, metadata=doc.metadata.copy()))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d9c6ef4",
   "metadata": {
    "executionInfo": {
     "elapsed": 19936,
     "status": "ok",
     "timestamp": 1748874288288,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "1d9c6ef4"
   },
   "outputs": [],
   "source": [
    "chunks = generate_chunks(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2118d7d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1748874301998,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "2118d7d4",
    "outputId": "90995001-e0f8-4b27-a269-7fab1cf0f28b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461091\n"
     ]
    }
   ],
   "source": [
    "print(len(chunks))  # Print the number of chunks generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LeAeTqpZ-DYw",
   "metadata": {
    "id": "LeAeTqpZ-DYw"
   },
   "source": [
    "## **2. Vector Database and RAG Chain Creation** <font color=red> [15 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YoH_Ac6K6aQZ",
   "metadata": {
    "id": "YoH_Ac6K6aQZ"
   },
   "source": [
    "### **2.1 Vector Embedding and Vector Database Creation** <font color=red> [7 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bBfj5ycC59lU",
   "metadata": {
    "id": "bBfj5ycC59lU"
   },
   "source": [
    "#### **2.1.1** <font color=red> [2 marks] </font>\n",
    "Initialise an embedding function for loading the embeddings into the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v-QeR5N_7jiw",
   "metadata": {
    "id": "v-QeR5N_7jiw"
   },
   "source": [
    "Initialise a function to transform the text to vectors using OPENAI Embeddings module. You can also use this function to transform during vector DB creation itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "purQgINbhpxO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2017,
     "status": "ok",
     "timestamp": 1748874310652,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "purQgINbhpxO",
    "outputId": "843f22f0-b186-46c7-b503-e6d692074d2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-2735c6b3b279>:17: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Embeddings initialized successfully.\n",
      "Embeddings object created and ready to use.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Replace with your actual OpenAI API key\n",
    "openai_api_key = \"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\"\n",
    "\n",
    "def get_openai_embeddings(api_key):\n",
    "    \"\"\"\n",
    "    Initialize the OpenAI Embeddings module with the provided API key.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Your OpenAI API key.\n",
    "\n",
    "    Returns:\n",
    "        OpenAIEmbeddings: An instance of the OpenAIEmbeddings class.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "        print(\"OpenAI Embeddings initialized successfully.\")\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing OpenAI Embeddings: {e}\")\n",
    "        return None\n",
    "\n",
    "embeddings = get_openai_embeddings(openai_api_key)\n",
    "\n",
    "if embeddings:\n",
    "    print(\"Embeddings object created and ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WTkTIerj5-KI",
   "metadata": {
    "id": "WTkTIerj5-KI"
   },
   "source": [
    "#### **2.1.2** <font color=red> [5 marks] </font>\n",
    "Load the embeddings to a vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o6rEbd7477R8",
   "metadata": {
    "id": "o6rEbd7477R8"
   },
   "source": [
    "Create a directory for vector database and enter embedding data to the vector DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28025350",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3280822,
     "status": "ok",
     "timestamp": 1748877597775,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "28025350",
    "outputId": "bbcba2ca-b237-4550-f364-421d558e08a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created and persisted at: vector_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-0b2153136717>:24: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_db.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "def create_vector_database(chunks, embeddings, persist_directory=\"vector_db\"):\n",
    "\n",
    "    try:\n",
    "        # Create the vector database\n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "\n",
    "        # Persist the vector database\n",
    "        vector_db.persist()\n",
    "        print(f\"Vector database created and persisted at: {persist_directory}\")\n",
    "        return vector_db\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector database: {e}\")\n",
    "        return None\n",
    "\n",
    "persist_directory = \"vector_db\"\n",
    "vector_db = create_vector_database(chunks, embeddings, persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978619ac",
   "metadata": {
    "id": "978619ac"
   },
   "source": [
    "### **2.2 Create RAG Chain** <font color=red> [8 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rczna1Xy_1bq",
   "metadata": {
    "id": "Rczna1Xy_1bq"
   },
   "source": [
    "#### **2.2.1** <font color=red> [5 marks] </font>\n",
    "Create a RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sEzxYN93Ygju",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1748877772860,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "sEzxYN93Ygju",
    "outputId": "dd69ea6d-07dd-4716-d64c-68966ade9cb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a RAG chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Assuming you have already initialized the embeddings and vector_db\n",
    "# from the previous steps.\n",
    "\n",
    "# 1. Initialize the Language Model\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# 2. Create a Retriever from the Vector Database\n",
    "retriever = vector_db.as_retriever()\n",
    "\n",
    "# 3. Create the RAG Chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # 'stuff' means all retrieved documents are stuffed into the prompt\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    "\n",
    "print(\"RAG chain created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6PkgzeTIElfy",
   "metadata": {
    "id": "6PkgzeTIElfy"
   },
   "source": [
    "#### **2.2.2** <font color=red> [3 marks] </font>\n",
    "Create a function to generate answer for asked questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W8AMtr94FZxR",
   "metadata": {
    "id": "W8AMtr94FZxR"
   },
   "source": [
    "Use the RAG chain to generate answer for a question and provide source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9TQdz5uFzlr",
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1748877777526,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "b9TQdz5uFzlr"
   },
   "outputs": [],
   "source": [
    "# Create a function for question answering\n",
    "\n",
    "def answer_question(question, rag_chain):\n",
    "    \"\"\"\n",
    "    Uses the RAG chain to generate an answer for a given question\n",
    "    and provides source documents if available.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to be answered.\n",
    "        rag_chain: The initialized RAG chain object.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the answer and source documents (if available).\n",
    "              Returns None if rag_chain is not initialized.\n",
    "    \"\"\"\n",
    "    if rag_chain is None:\n",
    "        print(\"RAG chain is not initialized.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Run the RAG chain with the question\n",
    "        response = rag_chain({\"query\": question})\n",
    "\n",
    "        # Extract the answer and source documents\n",
    "        answer = response.get(\"result\", \"Could not find an answer.\")\n",
    "        source_documents = response.get(\"source_documents\", [])\n",
    "\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"source_documents\": source_documents\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2BtpMz2LPTy5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2350,
     "status": "ok",
     "timestamp": 1748877803169,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "2BtpMz2LPTy5",
    "outputId": "f0abe79e-d890-4778-ee0b-4af0c8859ce1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-d0bdac63a787>:23: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = rag_chain({\"query\": question})\n"
     ]
    }
   ],
   "source": [
    "question = \"Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document indicate that the Agreement does not grant the Receiving Party any rights to the Confidential Information?\"\n",
    "result = answer_question(question, rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "iyc1I4TgPcGd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1748877918255,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "iyc1I4TgPcGd",
    "outputId": "7b1c9801-ff81-4c76-bd9a-40a851ad63d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the context provided, the document does not explicitly state whether the Agreement grants or does not grant the Receiving Party any rights to the Confidential Information. If this is a critical aspect of the agreement, it would be advisable to review the specific terms and clauses of the Non-Disclosure Agreement between CopAcc and ToP Mentors to determine if such a provision is included.\n",
      "\n",
      "Source Documents:\n",
      "Document 1:\n",
      "  Content: nondisclosure agreement keep in confidence and prevent the disclosure of proprietary information to any third party other than those of receiving partys i employees agents representatives directors...\n",
      "  Metadata: {'source': '/content/drive/MyDrive/Colab Notebooks/RAG Upgrad/rag_legal/corpus/contractnli/lti-two-way-cda-template.txt'}\n",
      "--------------------\n",
      "Document 2:\n",
      "  Content: prior nondisclosure secrecy or confidentiality agreement between the parties or their affiliates dealing with the subject of this agreement including the confidentiality agreement between the parties...\n",
      "  Metadata: {'source': '/content/drive/MyDrive/Colab Notebooks/RAG Upgrad/rag_legal/corpus/cuad/AimmuneTherapeuticsInc_20200205_8-K_EX-10.3_11967170_EX-10.3_Development Agreement.txt'}\n",
      "--------------------\n",
      "Document 3:\n",
      "  Content: a recipient in connection with performance of the services requires a specific form of nondisclosure agreement as a condition of such third partys consent to use the same for the benefit of recipient...\n",
      "  Metadata: {'source': '/content/drive/MyDrive/Colab Notebooks/RAG Upgrad/rag_legal/corpus/cuad/ReynoldsConsumerProductsInc_20200121_S-1A_EX-10.22_11948918_EX-10.22_Service Agreement.txt'}\n",
      "--------------------\n",
      "Document 4:\n",
      "  Content: 161 nonuse and nondisclosure during the agreement term and for thereafter a receiving party shall i treat confidential information provided by disclosing party as it would treat its own information...\n",
      "  Metadata: {'source': '/content/drive/MyDrive/Colab Notebooks/RAG Upgrad/rag_legal/corpus/cuad/FOUNDATIONMEDICINE,INC_02_02_2015-EX-10.2-Collaboration Agreement.txt'}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "if result is not None:\n",
    "    print(\"Answer:\", result[\"answer\"])\n",
    "    print(\"\\nSource Documents:\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"]):\n",
    "        print(f\"Document {i+1}:\")\n",
    "        print(f\"  Content: {doc.page_content[:200]}...\") # Print first 200 characters\n",
    "        print(f\"  Metadata: {doc.metadata}\")\n",
    "        print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fMmX8OrcN05D",
   "metadata": {
    "id": "fMmX8OrcN05D"
   },
   "source": [
    "## **3. RAG Evaluation** <font color=red> [10 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddfed9",
   "metadata": {
    "id": "b1ddfed9"
   },
   "source": [
    "### **3.1 Evaluation and Inference** <font color=red> [10 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z9xy_GduS9Yk",
   "metadata": {
    "id": "Z9xy_GduS9Yk"
   },
   "source": [
    "#### **3.1.1** <font color=red> [2 marks] </font>\n",
    "Extract all the questions and all the answers/ground truths from the benchmark files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V397RqkRfjSP",
   "metadata": {
    "id": "V397RqkRfjSP"
   },
   "source": [
    "Create a questions set and an answers set containing all the questions and answers from the benchmark files to run evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bF_KZXb1c-G5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2106,
     "status": "ok",
     "timestamp": 1748878014228,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "bF_KZXb1c-G5",
    "outputId": "bb5b2fd8-4a67-4e70-f776-57cf4a1d18f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 6856 unique questions.\n"
     ]
    }
   ],
   "source": [
    "# Create a question set by taking all the questions from the benchmark data\n",
    "# Also create a ground truth/answer set\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Define the base path for the benchmark data\n",
    "benchmark_base_path = \"/content/drive/MyDrive/Colab Notebooks/RAG Upgrad/rag_legal/benchmarks\"\n",
    "\n",
    "# List of benchmark JSON files\n",
    "benchmark_files = [\"contractnli.json\", \"cuad.json\", \"maud.json\", \"privacy_qa.json\"]\n",
    "\n",
    "# Set to store all unique questions\n",
    "question_set = set()\n",
    "\n",
    "# List to store all ground truth answers, organized by question\n",
    "ground_truth_answers = {} # Using a dictionary to map questions to their snippets\n",
    "\n",
    "# Iterate through each benchmark file\n",
    "for file_name in benchmark_files:\n",
    "    file_path = os.path.join(benchmark_base_path, file_name)\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            benchmark_data = json.load(f)\n",
    "\n",
    "        # Iterate through each test in the benchmark data\n",
    "        for test in benchmark_data.get(\"tests\", []):\n",
    "            query = test.get(\"query\")\n",
    "            snippets = test.get(\"snippets\", [])\n",
    "\n",
    "            if query:\n",
    "                # Add the question to the set\n",
    "                question_set.add(query)\n",
    "\n",
    "                # Store the snippets (ground truth answers) for this question\n",
    "                # We'll store the whole snippet list for each question\n",
    "                ground_truth_answers[query] = snippets\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Benchmark file not found: {file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON from file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
    "\n",
    "# Convert the question set to a list if you prefer\n",
    "question_list = list(question_set)\n",
    "\n",
    "print(f\"Extracted {len(question_list)} unique questions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81VscxuGTHhC",
   "metadata": {
    "id": "81VscxuGTHhC"
   },
   "source": [
    "#### **3.1.2** <font color=red> [5 marks] </font>\n",
    "Create a function to evaluate the generated answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.tokenize import word_tokenize # Ensure word_tokenize is imported\n",
    "import os # Import os module\n",
    "import pandas as pd # Import pandas\n",
    "\n",
    "# Download necessary NLTK data for BLEU if not already present\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Assume file_content_map is globally available and correctly populated earlier in the notebook\n",
    "# Example: Accessing the global file_content_map\n",
    "\n",
    "def evaluate_rag_performance(questions, ground_truth_answers, rag_chain, llm, embeddings, file_content_map):\n",
    "\n",
    "    if not rag_chain:\n",
    "        print(\"RAG chain is not initialized. Cannot perform evaluation.\")\n",
    "        return None\n",
    "\n",
    "    if not llm:\n",
    "        print(\"LLM is not initialized. Cannot perform Ragas evaluation.\")\n",
    "        ragas_results = \"LLM not initialized, Ragas metrics skipped.\"\n",
    "    elif not embeddings:\n",
    "         print(\"Embeddings are not initialized. Cannot perform Ragas evaluation.\")\n",
    "         ragas_results = \"Embeddings not initialized, Ragas metrics skipped.\"\n",
    "    else:\n",
    "        # --- Prepare data for Ragas ---\n",
    "        data_for_ragas = {\n",
    "            \"question\": [],\n",
    "            \"answer\": [],\n",
    "            \"contexts\": [],\n",
    "            \"ground_truths\": [],\n",
    "            \"reference\": [], # Modified for Ragas context_recall\n",
    "        }\n",
    "\n",
    "        print(\"\\nGathering data for Ragas evaluation...\")\n",
    "        base_corpus_path = \"/content/drive/MyDrive/Colab Notebooks/RAG Upgrad/rag_legal/corpus\" # Define the base path for corpus\n",
    "\n",
    "        # Iterate through the questions to evaluate\n",
    "        for q in questions:\n",
    "            # Get RAG answer and contexts for the current question\n",
    "            rag_response = answer_question(q, rag_chain)\n",
    "            generated_answer = rag_response.get(\"answer\", \"\") if rag_response else \"\"\n",
    "            # Ensure source_documents is a list before accessing page_content\n",
    "            retrieved_contexts = [doc.page_content for doc in rag_response.get(\"source_documents\", []) if hasattr(doc, 'page_content')]\n",
    "\n",
    "            # Get ground truth snippets for the current question\n",
    "            gt_snippets = ground_truth_answers.get(q, [])\n",
    "            ground_truths_list = []\n",
    "            references_text_list = [] # List to store reference texts for this question before joining\n",
    "\n",
    "            for snippet in gt_snippets:\n",
    "                answer = snippet.get(\"answer\", \"\")\n",
    "                file_path_suffix = snippet.get(\"file_path\", \"\")\n",
    "                span = snippet.get(\"span\")\n",
    "\n",
    "                if answer:\n",
    "                    ground_truths_list.append(answer)\n",
    "\n",
    "                # Extract reference text from original file content using file_path and span\n",
    "                if file_path_suffix and span and len(span) == 2:\n",
    "                    # Construct the full file path\n",
    "                    full_file_path = os.path.join(base_corpus_path, file_path_suffix)\n",
    "                    start, end = span\n",
    "\n",
    "                    # Get the original content using the file_content_map\n",
    "                    original_content = file_content_map.get(full_file_path, \"\")\n",
    "\n",
    "                    if original_content and 0 <= start < end <= len(original_content):\n",
    "                        reference_text = original_content[start:end]\n",
    "                         # Ensure the extracted text is a string and not empty before adding\n",
    "                        if isinstance(reference_text, str) and reference_text.strip():\n",
    "                            references_text_list.append(reference_text)\n",
    "\n",
    "\n",
    "            # Append data for this question to the lists\n",
    "\n",
    "            if ground_truths_list and references_text_list: # Proceed only if we have both ground truths and references\n",
    "                 data_for_ragas[\"question\"].append(q)\n",
    "                 data_for_ragas[\"answer\"].append(generated_answer)\n",
    "                 data_for_ragas[\"contexts\"].append(retrieved_contexts)\n",
    "                 data_for_ragas[\"ground_truths\"].append(ground_truths_list)\n",
    "                 # Join the list of reference texts into a single string\n",
    "                 # This is the change implemented in the previous step to address the 'valid string' error\n",
    "                 data_for_ragas[\"reference\"].append(\" \".join(references_text_list)) # Join with a space or other separator\n",
    "\n",
    "\n",
    "        # Convert to Ragas Dataset\n",
    "        # Check if there's any data to create a dataset\n",
    "        if data_for_ragas[\"question\"]:\n",
    "\n",
    "            ragas_dataset = Dataset.from_dict(data_for_ragas)\n",
    "\n",
    "            # Define Ragas metrics\n",
    "            metrics = [\n",
    "                faithfulness,\n",
    "                answer_relevancy,\n",
    "                context_recall,\n",
    "                context_precision,\n",
    "            ]\n",
    "\n",
    "            # Perform Ragas evaluation\n",
    "            try:\n",
    "                print(\"\\nStarting Ragas evaluation...\")\n",
    "                ragas_results = evaluate(\n",
    "                    ragas_dataset,\n",
    "                    metrics=metrics,\n",
    "                    llm=llm,\n",
    "                    embeddings=embeddings # Assuming embeddings is initialized globally or passed\n",
    "                )\n",
    "                print(\"Ragas evaluation finished.\")\n",
    "                # Convert to pandas DataFrame\n",
    "                ragas_results = ragas_results.to_pandas()\n",
    "                print(ragas_results)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during Ragas evaluation: {e}\")\n",
    "                # If Ragas evaluation fails, store the error message\n",
    "                ragas_results = f\"Error during Ragas evaluation: {e}\"\n",
    "        else:\n",
    "            # If no data collected for Ragas, store a message\n",
    "            ragas_results = \"No data collected for Ragas evaluation (ensure questions have ground truths and references).\"\n",
    "            print(ragas_results)\n",
    "\n",
    "\n",
    "    # --- Prepare data for ROUGE and BLEU ---\n",
    "\n",
    "\n",
    "    all_rouge_scores_rouge1 = []\n",
    "    all_rouge_scores_rougeL = []\n",
    "    all_bleu_scores = []\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    print(\"\\nCalculating ROUGE and BLEU scores...\")\n",
    "    for q in questions:\n",
    "        rag_response = answer_question(q, rag_chain)\n",
    "        generated_answer = rag_response[\"answer\"] if rag_response else \"\"\n",
    "\n",
    "        # Get ground truth answers for the current question\n",
    "        gt_snippets = ground_truth_answers.get(q, [])\n",
    "        gt_answers_for_q = [snippet.get(\"answer\", \"\") for snippet in gt_snippets if snippet.get(\"answer\")]\n",
    "\n",
    "        if generated_answer and gt_answers_for_q: # Only evaluate if we have both generated and ground truth answers\n",
    "            # ROUGE calculation\n",
    "            # Compare generated answer against all ground truths for this question\n",
    "            rouge_results_for_q = [scorer.score(gt_answer, generated_answer) for gt_answer in gt_answers_for_q]\n",
    "            # Collect fmeasure scores for averaging later\n",
    "            all_rouge_scores_rouge1.extend([r['rouge1'].fmeasure for r in rouge_results_for_q])\n",
    "            all_rouge_scores_rougeL.extend([r['rougeL'].fmeasure for r in rouge_results_for_q])\n",
    "\n",
    "\n",
    "            # BLEU calculation\n",
    "            # BLEU expects a list of reference sentences (ground truths) and a candidate sentence (generated answer)\n",
    "            # Tokenize sentences for BLEU\n",
    "            reference_tokenized = [word_tokenize(gt_ans) for gt_ans in gt_answers_for_q]\n",
    "            candidate_tokenized = word_tokenize(generated_answer)\n",
    "\n",
    "            # Calculate BLEU score - using smoothing function if needed for short texts\n",
    "            try:\n",
    "                # Calculate BLEU score for this question (against all references)\n",
    "                bleu_score = sentence_bleu(reference_tokenized, candidate_tokenized, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)\n",
    "                all_bleu_scores.append(bleu_score)\n",
    "            except Exception as e:\n",
    "                 print(f\"Error calculating BLEU for question: {q} - {e}\")\n",
    "                 all_bleu_scores.append(0) # Append 0 if BLEU calculation fails\n",
    "\n",
    "\n",
    "    # Aggregate ROUGE and BLEU scores across all questions and ground truths\n",
    "    # Calculate mean only if the list is not empty\n",
    "    avg_rouge_fmeasure_rouge1 = np.mean(all_rouge_scores_rouge1) if all_rouge_scores_rouge1 else 0\n",
    "    avg_rouge_fmeasure_rougeL = np.mean(all_rouge_scores_rougeL) if all_rouge_scores_rougeL else 0\n",
    "    avg_bleu_score = np.mean(all_bleu_scores) if all_bleu_scores else 0\n",
    "    print(\"ROUGE and BLEU calculation finished.\")\n",
    "\n",
    "\n",
    "    # Return all results\n",
    "    evaluation_results = {\n",
    "        \"ragas_scores\": ragas_results,\n",
    "        \"avg_rouge_fmeasure_rouge1\": avg_rouge_fmeasure_rouge1,\n",
    "        \"avg_rouge_fmeasure_rougeL\": avg_rouge_fmeasure_rougeL,\n",
    "        \"avg_bleu_score\": avg_bleu_score,\n",
    "    }\n",
    "\n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qIPUg1dKTPGb",
   "metadata": {
    "id": "qIPUg1dKTPGb"
   },
   "source": [
    "Evaluate the responses on *Rouge*, *Ragas* and *Bleu* scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ox4j4lLkbQp1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 830,
     "referenced_widgets": [
      "75d751eff64748c995e0c12cfac803cc",
      "2a26c07e259f4129b62d154ba20fc47b",
      "616bfb4786b54c479f56e6745fb179c7",
      "7bb63e9a1ec54c9fb2ec63bf233bcc9d",
      "5579bbd6848943939b4def1a065e43b9",
      "845210ce124e491c868bc69ee3c3c12f",
      "f3e0b732f27749a1b8ff3c668feb5bc4",
      "6fcd38b92a974366a634b7a7c3b50982",
      "7c01a8a0cf7b4539be5e0537deb87b05",
      "43af8d06615e4822869d41d2f657b899",
      "2488ba53cb264bab821e450f8bc6889a"
     ]
    },
    "executionInfo": {
     "elapsed": 22933,
     "status": "ok",
     "timestamp": 1748880944241,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "ox4j4lLkbQp1",
    "outputId": "40b41b07-7a54-439e-9c64-c187cc8e89bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gathering data for Ragas evaluation...\n",
      "\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d751eff64748c995e0c12cfac803cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragas evaluation finished.\n",
      "                                          user_input  \\\n",
      "0  Consider the Non-Disclosure Agreement between ...   \n",
      "1  Consider the Distributor Agreement between Com...   \n",
      "2  Consider the Consulting Agreement between Guns...   \n",
      "\n",
      "                                  retrieved_contexts  \\\n",
      "0  [to each other for purposes of this agreement ...   \n",
      "1  [distributor agreement 1 certification and ide...   \n",
      "2  [consulting llc consultant a virginia limited ...   \n",
      "\n",
      "                                            response  \\\n",
      "0  No, the document does not state that Confident...   \n",
      "1  The warranty period provided in the Distributo...   \n",
      "2  The governing law for the Consulting Agreement...   \n",
      "\n",
      "                                           reference  faithfulness  \\\n",
      "0  Confidential Information includes, without lim...           1.0   \n",
      "1  ITS will provide free technical support to cus...           0.0   \n",
      "2  This Agreement shall be interpreted, construed...           0.0   \n",
      "\n",
      "   answer_relevancy  context_recall  context_precision  \n",
      "0          0.936321             0.0                0.0  \n",
      "1          0.970968             0.0                0.0  \n",
      "2          0.979186             0.0                0.0  \n",
      "\n",
      "Calculating ROUGE and BLEU scores...\n",
      "ROUGE and BLEU calculation finished.\n",
      "\n",
      "--- Evaluation Results ---\n",
      "\n",
      "Ragas Scores:\n",
      "faithfulness         0.333333\n",
      "answer_relevancy     0.962158\n",
      "context_recall       0.000000\n",
      "context_precision    0.000000\n",
      "dtype: float64\n",
      "\n",
      "Average ROUGE-1 F1 Score: 0.2409\n",
      "Average ROUGE-L F1 Score: 0.1822\n",
      "Average BLEU Score: 0.0435\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Select a subset of questions for faster evaluation if needed\n",
    "questions_to_evaluate = question_list[:3] # Evaluate the first 3 questions\n",
    "\n",
    "# Pass file_content_map to the function\n",
    "evaluation_results = evaluate_rag_performance(questions_to_evaluate, ground_truth_answers, rag_chain, llm, embeddings, file_content_map)\n",
    "\n",
    "if evaluation_results:\n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    if isinstance(evaluation_results[\"ragas_scores\"], str):\n",
    "        # If ragas_scores is a string (error message), print it\n",
    "        print(evaluation_results[\"ragas_scores\"])\n",
    "    elif isinstance(evaluation_results[\"ragas_scores\"], pd.DataFrame):\n",
    "        # If ragas_scores is a DataFrame, calculate and print the mean of numeric columns\n",
    "        print(\"\\nRagas Scores:\")\n",
    "        # Select only numeric columns before calculating the mean\n",
    "        numeric_ragas_scores = evaluation_results[\"ragas_scores\"].select_dtypes(include=np.number)\n",
    "        print(numeric_ragas_scores.mean())\n",
    "    else:\n",
    "        # Handle other unexpected types if necessary\n",
    "        print(f\"Unexpected type for ragas_scores: {type(evaluation_results['ragas_scores'])}\")\n",
    "\n",
    "\n",
    "    print(f\"\\nAverage ROUGE-1 F1 Score: {evaluation_results['avg_rouge_fmeasure_rouge1']:.4f}\")\n",
    "    print(f\"Average ROUGE-L F1 Score: {evaluation_results['avg_rouge_fmeasure_rougeL']:.4f}\")\n",
    "    print(f\"Average BLEU Score: {evaluation_results['avg_bleu_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Omeb5vFSTbS0",
   "metadata": {
    "id": "Omeb5vFSTbS0"
   },
   "source": [
    "#### **3.1.3** <font color=red> [3 marks] </font>\n",
    "Draw inferences by evaluating answers to all questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ei2qIN71Tirg",
   "metadata": {
    "id": "ei2qIN71Tirg"
   },
   "source": [
    "To save time and computing power, you can just run the evaluation on first 100 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "pjGXlGQMSiDm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f15bdfe787ca414a939ceacd7f5e4b68",
      "c9a5dc52178248dabc2d612826371501",
      "1cf78c209c4f467cae0b1b253208eaa8",
      "8498d31e3c314dc7b6df5613588618ac",
      "7f89d2d931934c258b930663304c29c8",
      "96567e18595f4d168bf3e2362088d8a7",
      "6de323ec727c43d382121295aae29a8c",
      "e4956c9ca3814bca85ea663ff5c40aef",
      "95a64c1118dd4fea85673fdc33d30456",
      "6403cae47fd94c4fb44f75347f6e7b13",
      "0c398d1c244945f3bac7c62f9f264148"
     ]
    },
    "executionInfo": {
     "elapsed": 1154878,
     "status": "error",
     "timestamp": 1748882354192,
     "user": {
      "displayName": "Ho Trung Hieu",
      "userId": "16406323037207411115"
     },
     "user_tz": -420
    },
    "id": "pjGXlGQMSiDm",
    "outputId": "c21a65e5-632f-402a-fd9b-715e4c622968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for the first 100 questions...\n",
      "\n",
      "Gathering data for Ragas evaluation...\n",
      "\n",
      "Starting Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15bdfe787ca414a939ceacd7f5e4b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:ragas.executor:Exception raised in Job[15]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29807, Requested 1167. Please try again in 1.947s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[34]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29359, Requested 1256. Please try again in 1.23s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[23]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[26]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[27]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[43]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 30000, Requested 1990. Please try again in 3.979s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[42]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[44]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[47]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[58]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29894, Requested 1326. Please try again in 2.44s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[66]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29581, Requested 2428. Please try again in 4.017s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[50]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[54]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29931, Requested 1729. Please try again in 3.32s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[55]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[67]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[78]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 28824, Requested 1482. Please try again in 612ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[79]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[106]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 30000, Requested 1623. Please try again in 3.246s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[94]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29073, Requested 1329. Please try again in 804ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[98]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[107]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[108]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[110]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[111]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[135]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29593, Requested 1723. Please try again in 2.632s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[114]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[115]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[126]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29389, Requested 1484. Please try again in 1.746s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[134]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29426, Requested 1819. Please try again in 2.49s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[127]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 28996, Requested 1387. Please try again in 766ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[142]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 30000, Requested 1327. Please try again in 2.654s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[138]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[150]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29651, Requested 1418. Please try again in 2.138s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[151]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[158]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29203, Requested 1203. Please try again in 812ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[160]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[178]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 30000, Requested 1222. Please try again in 2.444s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[182]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29266, Requested 1219. Please try again in 970ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[186]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 30000, Requested 1291. Please try again in 2.582s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[190]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 30000, Requested 1249. Please try again in 2.498s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[199]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29287, Requested 1195. Please try again in 964ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[202]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29345, Requested 1313. Please try again in 1.316s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[210]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29942, Requested 1298. Please try again in 2.48s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[203]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[218]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29095, Requested 1257. Please try again in 704ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[222]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29101, Requested 1529. Please try again in 1.26s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[220]: TimeoutError()\n",
      "ERROR:ragas.executor:Exception raised in Job[223]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 30000, Requested 1431. Please try again in 2.862s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[224]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29533, Requested 1299. Please try again in 1.664s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[234]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 30000, Requested 1268. Please try again in 2.536s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[250]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 28953, Requested 1571. Please try again in 1.048s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[254]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 29841, Requested 1337. Please try again in 2.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "ERROR:ragas.executor:Exception raised in Job[248]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-S46hVSjhX3sfimLNnR31ofXI on tokens per min (TPM): Limit 30000, Used 30000, Requested 1400. Please try again in 2.8s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-92f5ced0d9c7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Call the evaluate_rag_performance function with the list of questions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# This function is designed to handle multiple questions and calculate aggregate scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m evaluation_results = evaluate_rag_performance(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mquestions_to_evaluate\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;31m# Pass the list of questions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mground_truth_answers\u001b[0m\u001b[0;34m,\u001b[0m      \u001b[0;31m# Pass the dictionary of ground truth answers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-00567c22e49b>\u001b[0m in \u001b[0;36mevaluate_rag_performance\u001b[0;34m(questions, ground_truth_answers, rag_chain, llm, embeddings, file_content_map)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting Ragas evaluation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 ragas_results = evaluate(\n\u001b[0m\u001b[1;32m    132\u001b[0m                     \u001b[0mragas_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragas/_analytics.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIsCompleteEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_completed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIsCompleteEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_completed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragas/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;31m# get the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mExceptionInRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ragas/executor.py\u001b[0m in \u001b[0;36mresults\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nest_asyncio_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0msorted_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 scheduled[0]._when - self.time(), 0), 86400) if scheduled\n\u001b[1;32m    114\u001b[0m             else None)\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mevent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run evaluation on the first 100 questions\n",
    "\n",
    "num_questions_to_evaluate = 100\n",
    "\n",
    "# Select the first N questions from the full list\n",
    "questions_to_evaluate = question_list[:num_questions_to_evaluate]\n",
    "\n",
    "print(f\"Starting evaluation for the first {len(questions_to_evaluate)} questions...\")\n",
    "\n",
    "# Call the evaluate_rag_performance function with the list of questions\n",
    "# This function is designed to handle multiple questions and calculate aggregate scores\n",
    "evaluation_results = evaluate_rag_performance(\n",
    "    questions_to_evaluate,       # Pass the list of questions\n",
    "    ground_truth_answers,      # Pass the dictionary of ground truth answers\n",
    "    rag_chain,                 # Pass the rag_chain object\n",
    "    llm,                       # Pass the llm object\n",
    "    embeddings,                # Pass the embeddings object\n",
    "    file_content_map           # Pass the file_content_map\n",
    ")\n",
    "\n",
    "# Now `evaluation_results` contains the aggregate scores (Ragas, ROUGE, BLEU)\n",
    "# for the batch of questions.\n",
    "\n",
    "print(\"\\n--- Aggregate Evaluation Results ---\")\n",
    "if evaluation_results:\n",
    "    if isinstance(evaluation_results[\"ragas_scores\"], str):\n",
    "        # If ragas_scores is a string (error message), print it\n",
    "        print(evaluation_results[\"ragas_scores\"])\n",
    "    elif isinstance(evaluation_results[\"ragas_scores\"], pd.DataFrame):\n",
    "        # If ragas_scores is a DataFrame, calculate and print the mean of numeric columns\n",
    "        print(\"\\nRagas Scores (Average):\")\n",
    "        # Select only numeric columns before calculating the mean\n",
    "        numeric_ragas_scores = evaluation_results[\"ragas_scores\"].select_dtypes(include=np.number)\n",
    "        print(numeric_ragas_scores.mean())\n",
    "        # Optional: Print the full Ragas DataFrame\n",
    "        # print(\"\\nFull Ragas Results DataFrame:\")\n",
    "        # print(evaluation_results[\"ragas_scores\"])\n",
    "    else:\n",
    "        # Handle other unexpected types if necessary\n",
    "        print(f\"Unexpected type for ragas_scores: {type(evaluation_results['ragas_scores'])}\")\n",
    "\n",
    "\n",
    "    print(f\"\\nAverage ROUGE-1 F1 Score: {evaluation_results['avg_rouge_fmeasure_rouge1']:.4f}\")\n",
    "    print(f\"Average ROUGE-L F1 Score: {evaluation_results['avg_rouge_fmeasure_rougeL']:.4f}\")\n",
    "    print(f\"Average BLEU Score: {evaluation_results['avg_bleu_score']:.4f}\")\n",
    "else:\n",
    "    print(\"Evaluation did not produce results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gonMO9wNE5dt",
   "metadata": {
    "id": "gonMO9wNE5dt"
   },
   "source": [
    "## **4. Conclusion** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ySHPR29rE5du",
   "metadata": {
    "id": "ySHPR29rE5du"
   },
   "source": [
    "### **4.1 Conclusions and insights** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KoVsmcV0E5du",
   "metadata": {
    "id": "KoVsmcV0E5du"
   },
   "source": [
    "#### **4.1.1** <font color=red> [5 marks] </font>\n",
    "Conclude with the results here. Include the insights gained about the data, model pipeline, the RAG process and the results obtained.\n",
    "\n",
    "Based on the steps taken and the results observed, here are some insights regarding the data, the model pipeline, the RAG process, and the evaluation:\n",
    "\n",
    "### **4.1.1 Data Understanding and Preparation**\n",
    "\n",
    "*   **Data Source and Structure:** The dataset consists of legal documents in `.txt` format, organized into folders by type (`contractnli`, `cuad`, `maud`, `privacy_qa`). The benchmark data provides structured questions and ground truth answers with file paths and spans, which is crucial for detailed evaluation, especially for metrics like Ragas's `context_recall`.\n",
    "*   **Loading Challenges:** While `TextLoader` simplifies loading, handling various file encodings or potential parsing errors requires robust error handling (as implemented with the `try-except` block).\n",
    "*   **Preprocessing Importance:** The preprocessing steps (removing emails, phone numbers, special characters, lowercasing) are essential for cleaning noise and standardizing the text. This helps in subsequent steps like TF-IDF calculation and embedding generation by focusing on meaningful content.\n",
    "*   **`file_content_map`:** Creating and maintaining a map from file paths to *original* content is critical for evaluation metrics like Ragas's `context_recall`, which requires comparing the retrieved context chunks back to the exact spans in the original ground truth documents. Preprocessing text *before* creating this map would hinder accurate span extraction from the original files. (Note: The current code preprocesses *before* storing in `file_content_map`, which is an area for potential refinement if exact span matching on original text is needed for certain metrics).\n",
    "\n",
    "### **4.1.2 Exploratory Data Analysis (EDA)**\n",
    "\n",
    "*   **Document Length:** Analyzing document length provides a basic understanding of the scale of the data and informs chunking strategies. Legal documents can be quite long, necessitating effective splitting.\n",
    "*   **Word Frequency:** Identifying common and rare words (after removing stop words) gives insight into the dominant themes and domain-specific language within the legal corpus. The most common words likely relate to contract terms, legal entities, etc.\n",
    "*   **TF-IDF Similarity:** TF-IDF helps capture the importance of words in documents relative to the corpus. Calculating the similarity matrix (e.g., using cosine similarity) reveals how similar documents are in terms of their vocabulary and key terms.\n",
    "    *   Observing higher average similarity among the first 10 documents compared to a random set might suggest some level of inherent ordering or grouping in the initial dataset load, or simply random chance. In a real-world scenario, analysing similarity across different document types (folders) could reveal more structural insights. Detecting highly similar documents or clauses could be useful for de-duplication or identifying standard boilerplate language.\n",
    "\n",
    "### **4.1.3 Document Creation and Chunking**\n",
    "\n",
    "*   **Necessity of Chunking:** Legal documents are often too large to fit into the context window of modern language models. Chunking breaks them down into manageable pieces while retaining necessary information and some surrounding context (using `chunk_overlap`).\n",
    "*   **`RecursiveCharacterTextSplitter`:** This splitter is effective for handling various separators (newlines, spaces) to create logical text chunks. The choice of `chunk_size` and `chunk_overlap` is a hyperparameter that significantly impacts RAG performance – too small, and context is lost; too large, and irrelevant information is included or context windows are exceeded. The chosen values (200/50) are relatively small and might need tuning for a legal domain, which often requires more context.\n",
    "*   **Maintaining Metadata:** Ensuring metadata (like the original file name/path) is carried over to each chunk is crucial for traceability and for linking retrieved chunks back to their source documents during evaluation and for providing sources in the final answer.\n",
    "\n",
    "### **4.1.4 Vector Database and RAG Chain Creation**\n",
    "\n",
    "*   **Embeddings:** Embeddings transform text chunks into numerical vectors, allowing semantic similarity search. `OpenAIEmbeddings` provides a powerful pre-trained model, but requires an API key.\n",
    "*   **Vector Database (Chroma):** Chroma serves as an efficient store for vector embeddings and their associated metadata. It enables fast retrieval of relevant chunks based on the query's embedding similarity. Persisting the database (`persist_directory`) saves time by avoiding re-embedding the corpus every time the application runs.\n",
    "*   **RAG Chain (`RetrievalQA`):** The RAG chain connects the retriever (vector DB) with the language model (LLM). The retriever fetches relevant chunks based on the query, and the LLM then synthesizes an answer using the query and the retrieved context. The 'stuff' chain type is simple but can be problematic if the total context length (query + retrieved chunks) exceeds the LLM's context window. Other chain types (`map_reduce`, `refine`) could be explored for very large contexts.\n",
    "*   **LLM Choice:** The choice of `gpt-4o` is a powerful one for generating coherent and relevant answers, given sufficient context. Setting `temperature=0` promotes more deterministic and factual responses, which is often desirable in legal contexts.\n",
    "\n",
    "### **4.1.5 RAG Evaluation**\n",
    "\n",
    "*   **Importance of Evaluation:** Evaluating the RAG system is vital to understand its effectiveness beyond anecdotal examples. Metrics quantify different aspects of performance.\n",
    "*   **Benchmark Data Usage:** Leveraging the structured benchmark data to extract questions and ground truths is the foundation for automated evaluation.\n",
    "*   **Metric Interpretation:**\n",
    "    *   **Ragas:** Provides nuanced evaluation by assessing attributes like whether the generated answer is grounded in the retrieved context (`faithfulness`), whether the answer is relevant to the question (`answer_relevancy`), and how well the retrieved context covers the information needed to answer the question (`context_recall`) and is free of irrelevant information (`context_precision`).\n",
    "    *   **ROUGE & BLEU:** These are traditional text generation metrics comparing the output directly to reference answers based on word overlap. While useful, they don't specifically evaluate the *retrieval* aspect of RAG or whether the answer is *supported* by the retrieved context.\n",
    "*   **Evaluation Setup Challenges:** Correctly preparing the data format for evaluation libraries like Ragas (especially linking ground truths to their original reference text spans for context recall) requires careful data manipulation. The error encountered highlights the importance of matching function signatures and data structures.\n",
    "\n",
    "### **4.1.6 Overall Conclusion**\n",
    "\n",
    "Building a RAG system involves several interconnected steps, each impacting the final performance. Data loading, preprocessing, chunking, embedding choice, vector database configuration, and the RAG chain structure all play a role. Evaluation using a combination of metrics like Ragas, ROUGE, and BLEU provides a multi-faceted view of the system's strengths and weaknesses. The initial evaluation provides a baseline, and further optimization of chunking, retrieval parameters (e.g., number of documents to retrieve), and LLM prompting techniques would likely be necessary to improve the RAG system's ability to accurately answer questions based on the legal corpus. The ability to link ground truth answers to original document spans is a powerful feature for pinpointing exactly where relevant information *should* have been found by the retriever for accurate `context_recall` measurement.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "LeAeTqpZ-DYw",
    "fMmX8OrcN05D",
    "gonMO9wNE5dt"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c398d1c244945f3bac7c62f9f264148": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1cf78c209c4f467cae0b1b253208eaa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4956c9ca3814bca85ea663ff5c40aef",
      "max": 376,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_95a64c1118dd4fea85673fdc33d30456",
      "value": 287
     }
    },
    "2488ba53cb264bab821e450f8bc6889a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a26c07e259f4129b62d154ba20fc47b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_845210ce124e491c868bc69ee3c3c12f",
      "placeholder": "​",
      "style": "IPY_MODEL_f3e0b732f27749a1b8ff3c668feb5bc4",
      "value": "Evaluating: 100%"
     }
    },
    "43af8d06615e4822869d41d2f657b899": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5579bbd6848943939b4def1a065e43b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "616bfb4786b54c479f56e6745fb179c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fcd38b92a974366a634b7a7c3b50982",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7c01a8a0cf7b4539be5e0537deb87b05",
      "value": 12
     }
    },
    "6403cae47fd94c4fb44f75347f6e7b13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6de323ec727c43d382121295aae29a8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fcd38b92a974366a634b7a7c3b50982": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75d751eff64748c995e0c12cfac803cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2a26c07e259f4129b62d154ba20fc47b",
       "IPY_MODEL_616bfb4786b54c479f56e6745fb179c7",
       "IPY_MODEL_7bb63e9a1ec54c9fb2ec63bf233bcc9d"
      ],
      "layout": "IPY_MODEL_5579bbd6848943939b4def1a065e43b9"
     }
    },
    "7bb63e9a1ec54c9fb2ec63bf233bcc9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43af8d06615e4822869d41d2f657b899",
      "placeholder": "​",
      "style": "IPY_MODEL_2488ba53cb264bab821e450f8bc6889a",
      "value": " 12/12 [00:12&lt;00:00,  1.20s/it]"
     }
    },
    "7c01a8a0cf7b4539be5e0537deb87b05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7f89d2d931934c258b930663304c29c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "845210ce124e491c868bc69ee3c3c12f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8498d31e3c314dc7b6df5613588618ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6403cae47fd94c4fb44f75347f6e7b13",
      "placeholder": "​",
      "style": "IPY_MODEL_0c398d1c244945f3bac7c62f9f264148",
      "value": " 287/376 [15:55&lt;05:12,  3.51s/it]"
     }
    },
    "95a64c1118dd4fea85673fdc33d30456": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "96567e18595f4d168bf3e2362088d8a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9a5dc52178248dabc2d612826371501": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96567e18595f4d168bf3e2362088d8a7",
      "placeholder": "​",
      "style": "IPY_MODEL_6de323ec727c43d382121295aae29a8c",
      "value": "Evaluating:  76%"
     }
    },
    "e4956c9ca3814bca85ea663ff5c40aef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f15bdfe787ca414a939ceacd7f5e4b68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c9a5dc52178248dabc2d612826371501",
       "IPY_MODEL_1cf78c209c4f467cae0b1b253208eaa8",
       "IPY_MODEL_8498d31e3c314dc7b6df5613588618ac"
      ],
      "layout": "IPY_MODEL_7f89d2d931934c258b930663304c29c8"
     }
    },
    "f3e0b732f27749a1b8ff3c668feb5bc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
