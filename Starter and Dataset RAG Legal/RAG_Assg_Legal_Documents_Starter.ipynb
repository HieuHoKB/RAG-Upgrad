{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lf5lYawIw8tE",
   "metadata": {
    "id": "lf5lYawIw8tE"
   },
   "source": [
    "# **Extracting Information from Legal Documents Using RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NY1InIbkw80B",
   "metadata": {
    "id": "NY1InIbkw80B"
   },
   "source": [
    "## **Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3403a4b5",
   "metadata": {
    "id": "3403a4b5"
   },
   "source": [
    "The main objective of this assignment is to process and analyse a collection text files containing legal agreements (e.g., NDAs) to prepare them for implementing a **Retrieval-Augmented Generation (RAG)** system. This involves:\n",
    "\n",
    "* Understand the Cleaned Data : Gain a comprehensive understanding of the structure, content, and context of the cleaned dataset.\n",
    "* Perform Exploratory Analysis : Conduct bivariate and multivariate analyses to uncover relationships and trends within the cleaned data.\n",
    "* Create Visualisations : Develop meaningful visualisations to support the analysis and make findings interpretable.\n",
    "* Derive Insights and Conclusions : Extract valuable insights from the cleaned data and provide clear, actionable conclusions.\n",
    "* Document the Process : Provide a detailed description of the data, its attributes, and the steps taken during the analysis for reproducibility and clarity.\n",
    "\n",
    "The ultimate goal is to transform the raw text data into a clean, structured, and analysable format that can be effectively used to build and train a RAG system for tasks like information retrieval, question-answering, and knowledge extraction related to legal agreements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3TTEcbb5hIM-",
   "metadata": {
    "id": "3TTEcbb5hIM-"
   },
   "source": [
    "### **Business Value**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZsfkEL2CgljF",
   "metadata": {
    "id": "ZsfkEL2CgljF"
   },
   "source": [
    "The project aims to leverage RAG to enhance legal document processing for businesses, law firms, and regulatory bodies. The key business objectives include:\n",
    "\n",
    "* Faster Legal Research: <br> Reduce the time lawyers and compliance officers spend searching for relevant case laws, precedents, statutes, or contract clauses.\n",
    "* Improved Contract Analysis: <br> Automatically extract key terms, obligations, and risks from lengthy contracts.\n",
    "* Regulatory Compliance Monitoring: <br> Help businesses stay updated with legal and regulatory changes by retrieving relevant legal updates.\n",
    "* Enhanced Decision-Making: <br> Provide accurate and context-aware legal insights to assist in risk assessment and legal strategy.\n",
    "\n",
    "\n",
    "**Use Cases**\n",
    "* Legal Chatbots\n",
    "* Contract Review Automation\n",
    "* Tracking Regulatory Changes and Compliance Monitoring\n",
    "* Case Law Analysis of past judgments\n",
    "* Due Diligence & Risk Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rDp_EWxVOhUu",
   "metadata": {
    "id": "rDp_EWxVOhUu"
   },
   "source": [
    "## **1. Data Loading, Preparation and Analysis** <font color=red> [20 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JZGTCfyUxalZ",
   "metadata": {
    "id": "JZGTCfyUxalZ"
   },
   "source": [
    "### **1.1 Data Understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ok6sSYNAiG8V",
   "metadata": {
    "id": "ok6sSYNAiG8V"
   },
   "source": [
    "The dataset contains legal documents and contracts collected from various sources. The documents are present as text files (`.txt`) in the *corpus* folder.\n",
    "\n",
    "There are four types of documents in the *courpus* folder, divided into four subfolders.\n",
    "- `contractnli`: contains various non-disclosure and confidentiality agreements\n",
    "- `cuad`: contains contracts with annotated legal clauses\n",
    "- `maud`: contains various merger/acquisition contracts and agreements\n",
    "- `privacy_qa`: a question-answering dataset containing privacy policies\n",
    "\n",
    "The dataset also contains evaluation files in JSON format in the *benchmark* folder. The files contain the questions and their answers, along with sources. For each of the above four folders, there is a `json` file: `contractnli.json`, `cuad.json`, `maud.json` `privacy_qa.json`. The file structure is as follows:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"tests\": [\n",
    "        {\n",
    "            \"query\": <question1>,\n",
    "            \"snippets\": [{\n",
    "                    \"file_path\": <source_file1>,\n",
    "                    \"span\": [ begin_position, end_position ],\n",
    "                    \"answer\": <relevant answer to the question 1>\n",
    "                },\n",
    "                {\n",
    "                    \"file_path\": <source_file2>,\n",
    "                    \"span\": [ begin_position, end_position ],\n",
    "                    \"answer\": <relevant answer to the question 2>\n",
    "                }, ....\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"query\": <question2>,\n",
    "            \"snippets\": [{<answer context for que 2>}]\n",
    "        },\n",
    "        ... <more queries>\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S7Ac8VxvjWnw",
   "metadata": {
    "id": "S7Ac8VxvjWnw"
   },
   "source": [
    "### **1.2 Load and Preprocess the data** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gJ8fA4Nh3fHg",
   "metadata": {
    "id": "gJ8fA4Nh3fHg"
   },
   "source": [
    "#### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "BqyFHhSn48tC",
   "metadata": {
    "id": "BqyFHhSn48tC"
   },
   "outputs": [],
   "source": [
    "## The following libraries might be useful\n",
    "!pip install -q langchain-openai\n",
    "!pip install -U -q langchain-community\n",
    "!pip install -U -q langchain-chroma\n",
    "!pip install -U -q datasets\n",
    "!pip install -U -q ragas\n",
    "!pip install -U -q rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Qpn-qbhAi58F",
   "metadata": {
    "id": "Qpn-qbhAi58F"
   },
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zOMf-tfIiOlp",
   "metadata": {
    "id": "zOMf-tfIiOlp"
   },
   "source": [
    "#### **1.2.1** <font color=red> [3 marks] </font>\n",
    "Load all `.txt` files from the folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea36ba",
   "metadata": {
    "id": "f2ea36ba"
   },
   "source": [
    "You can utilise document loaders from the options provided by the LangChain community.\n",
    "\n",
    "Optionally, you can also read the files manually, while ensuring proper handling of encoding issues (e.g., utf-8, latin1). In such case, also store the file content along with metadata (e.g., file name, directory path) for traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "I9rTY8DWx2Wj",
   "metadata": {
    "id": "I9rTY8DWx2Wj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 95 documents from contractnli\n",
      "Loaded 462 documents from cuad\n",
      "Loaded 134 documents from maud\n",
      "Loaded 7 documents from privacy_qa\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "# Define the base path for the corpus\n",
    "base_path = \"/Users/tien-nguyen/Downloads/RAG Upgrad/Starter and Dataset RAG Legal/rag_legal/corpus\"\n",
    "# Dictionary to store the loaded documents\n",
    "documents = {}\n",
    "\n",
    "# Iterate through the subfolders\n",
    "for folder in [\"contractnli\", \"cuad\", \"maud\", \"privacy_qa\"]:\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    documents[folder] = []\n",
    "    \n",
    "    # Load all .txt files in the folder using TextLoader\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            try:\n",
    "                loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "                docs = loader.load()\n",
    "                for doc in docs:\n",
    "                    documents[folder].append(doc)\n",
    "                #         \"page_content\": doc[\"page_content\"],\n",
    "                #         \"metadata\": {\"file_name\": file_name}\n",
    "                #     })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {file_path}: {e}\")\n",
    "\n",
    "# Example: Print the number of documents loaded for each folder\n",
    "for folder, docs in documents.items():\n",
    "    print(f\"Loaded {len(docs)} documents from {folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb384152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(docs[0]))  # Check the type of the first element in docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df394c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '/Users/tien-nguyen/Downloads/RAG Upgrad/Starter and Dataset RAG Legal/rag_legal/corpus/privacy_qa/Groupon.txt'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata  # Access the metadata of the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K4HYLoUjwmMs",
   "metadata": {
    "id": "K4HYLoUjwmMs"
   },
   "source": [
    "#### **1.2.2** <font color=red> [2 marks] </font>\n",
    "Preprocess the text data to remove noise and prepare it for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9793fdf",
   "metadata": {
    "id": "e9793fdf"
   },
   "source": [
    "Remove special characters, extra whitespace, and irrelevant content such as email and telephone contact info.\n",
    "Normalise text (e.g., convert to lowercase, remove stop words).\n",
    "Handle missing or corrupted data by logging errors and skipping problematic files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec87e69",
   "metadata": {
    "id": "1ec87e69"
   },
   "outputs": [],
   "source": [
    "# Clean and preprocess the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53d44537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by removing special characters, extra whitespace,\n",
    "    and irrelevant content such as email and telephone contact info.\n",
    "    Normalize text by converting to lowercase and removing stop words.\n",
    "    \"\"\"\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove phone numbers\n",
    "    text = re.sub(r'\\b\\d{10,}\\b', '', text)\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "for folder, docs in documents.items():\n",
    "    for doc in docs:\n",
    "        doc.page_content = preprocess_text(doc.page_content)\n",
    "\n",
    "print(\"Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e90470",
   "metadata": {
    "id": "b9e90470"
   },
   "source": [
    "### **1.3 Exploratory Data Analysis** <font color=red> [10 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Nd1K4yhIzyPp",
   "metadata": {
    "id": "Nd1K4yhIzyPp"
   },
   "source": [
    "#### **1.3.1** <font color=red> [1 marks] </font>\n",
    "Calculate the average, maximum and minimum document length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "tQT1UIcOHSp9",
   "metadata": {
    "id": "tQT1UIcOHSp9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Document Length: 99063.25787965616\n",
      "Maximum Document Length: 957212\n",
      "Minimum Document Length: 1329\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average, maximum, and minimum document length\n",
    "def calculate_document_lengths(documents):\n",
    "    lengths = []\n",
    "    for folder, docs in documents.items():\n",
    "        for doc in docs:\n",
    "            lengths.append(len(doc.page_content))  # Access \"page_content\" as a dictionary key\n",
    "\n",
    "    if lengths:\n",
    "        avg_length = sum(lengths) / len(lengths)\n",
    "        max_length = max(lengths)\n",
    "        min_length = min(lengths)\n",
    "        return avg_length, max_length, min_length\n",
    "    else:\n",
    "        return 0, 0, 0\n",
    "\n",
    "# Example usage\n",
    "avg_length, max_length, min_length = calculate_document_lengths(documents)\n",
    "print(f\"Average Document Length: {avg_length}\")\n",
    "print(f\"Maximum Document Length: {max_length}\")\n",
    "print(f\"Minimum Document Length: {min_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18xQu__O0wLv",
   "metadata": {
    "id": "18xQu__O0wLv"
   },
   "source": [
    "#### **1.3.2** <font color=red> [4 marks] </font>\n",
    "Analyse the frequency of occurrence of words and find the most and least occurring words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IQ_i5YfFH2dg",
   "metadata": {
    "id": "IQ_i5YfFH2dg"
   },
   "source": [
    "Find the 20 most common and least common words in the text. Ignore stop words such as articles and prepositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "Q8eiDTy2Ic8z",
   "metadata": {
    "id": "Q8eiDTy2Ic8z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tien-\n",
      "[nltk_data]     nguyen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/tien-\n",
      "[nltk_data]     nguyen/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/tien-\n",
      "[nltk_data]     nguyen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Most Common Words: [('company', 148167), ('shall', 107995), ('agreement', 104559), ('section', 75344), ('parent', 58009), ('party', 49656), ('date', 39281), ('time', 35251), ('material', 34208), ('merger', 33843), ('subsidiaries', 33317), ('applicable', 31368), ('including', 29398), ('respect', 28848), ('may', 28069), ('stock', 26651), ('information', 25681), ('parties', 24609), ('b', 23935), ('business', 23497)]\n",
      "20 Least Common Words: [('enrich', 1), ('simplify', 1), ('devicekeep', 1), ('appwe', 1), ('cached', 1), ('usernames', 1), ('wwwaboutcookiesorg', 1), ('safari', 1), ('personalise', 1), ('effortless', 1), ('signin', 1), ('customise', 1), ('gigs', 1), ('gig', 1), ('systemoperating', 1), ('deviceoperating', 1), ('countrycontinent', 1), ('weblog', 1), ('eat', 1), ('policypdf', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Find frequency of occurence of words\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')  # Ensure punkt_tab is downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a function to find the most and least common words\n",
    "def find_common_words(documents, num_words=20):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_counts = Counter()\n",
    "\n",
    "    # Tokenize and count words in all documents\n",
    "    for folder, docs in documents.items():\n",
    "        for doc in docs:\n",
    "            words = word_tokenize(doc.page_content)\n",
    "            filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "            word_counts.update(filtered_words)\n",
    "\n",
    "    # Get the most and least common words\n",
    "    most_common = word_counts.most_common(num_words)\n",
    "    least_common = word_counts.most_common()[:-num_words-1:-1]\n",
    "\n",
    "    return most_common, least_common\n",
    "\n",
    "# Example usage\n",
    "most_common, least_common = find_common_words(documents)\n",
    "print(\"20 Most Common Words:\", most_common)\n",
    "print(\"20 Least Common Words:\", least_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xlF55RNjz9pQ",
   "metadata": {
    "id": "xlF55RNjz9pQ"
   },
   "source": [
    "#### **1.3.3** <font color=red> [4 marks] </font>\n",
    "Analyse the similarity of different documents to each other based on TF-IDF vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jciCNMelOGPJ",
   "metadata": {
    "id": "jciCNMelOGPJ"
   },
   "source": [
    "Transform some documents to TF-IDF vectors and calculate their similarity matrix using a suitable distance function. If contracts contain duplicate or highly similar clauses, similarity calculation can help detect them.\n",
    "\n",
    "Identify for the first 10 documents and then for 10 random documents. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "M-_SrvDcMnKi",
   "metadata": {
    "id": "M-_SrvDcMnKi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (698, 53814)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Function to transform documents into TF-IDF vectors\n",
    "def transform_to_tfidf(documents):\n",
    "    # Combine all document contents into a list\n",
    "    all_docs = []\n",
    "    for folder, docs in documents.items():\n",
    "        for doc in docs:\n",
    "            all_docs.append(doc.page_content)\n",
    "\n",
    "    # Initialize the TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_docs)\n",
    "\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "# Example usage\n",
    "tfidf_matrix, vectorizer = transform_to_tfidf(documents)\n",
    "\n",
    "# Print the shape of the TF-IDF matrix\n",
    "print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73600f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrix (First 10 Documents):\n",
      "[[1.         0.6997468  0.8511427  0.6997468  0.79425781 0.66438238\n",
      "  0.76982506 0.68768813 0.75082418 0.74143225]\n",
      " [0.6997468  1.         0.75729146 1.         0.69855627 0.59016426\n",
      "  0.69249744 0.6425857  0.69853589 0.70027608]\n",
      " [0.8511427  0.75729146 1.         0.75729146 0.89258414 0.67856654\n",
      "  0.87353035 0.73556672 0.86221725 0.83430655]\n",
      " [0.6997468  1.         0.75729146 1.         0.69855627 0.59016426\n",
      "  0.69249744 0.6425857  0.69853589 0.70027608]\n",
      " [0.79425781 0.69855627 0.89258414 0.69855627 1.         0.64830348\n",
      "  0.9142389  0.72640849 0.87030152 0.798751  ]\n",
      " [0.66438238 0.59016426 0.67856654 0.59016426 0.64830348 1.\n",
      "  0.62882171 0.58418373 0.6537631  0.62822574]\n",
      " [0.76982506 0.69249744 0.87353035 0.69249744 0.9142389  0.62882171\n",
      "  1.         0.68872373 0.8490064  0.77839615]\n",
      " [0.68768813 0.6425857  0.73556672 0.6425857  0.72640849 0.58418373\n",
      "  0.68872373 1.         0.7034693  0.67288339]\n",
      " [0.75082418 0.69853589 0.86221725 0.69853589 0.87030152 0.6537631\n",
      "  0.8490064  0.7034693  1.         0.80474444]\n",
      " [0.74143225 0.70027608 0.83430655 0.70027608 0.798751   0.62822574\n",
      "  0.77839615 0.67288339 0.80474444 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity scores for 10 first documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Function to compute similarity scores for the first 10 documents\n",
    "# def compute_similarity_for_first_10(documents):\n",
    "#     # Combine all document contents into a list\n",
    "#     all_docs = []\n",
    "#     for folder, docs in documents.items():\n",
    "#         for doc in docs:\n",
    "#             all_docs.append(doc[\"page_content\"])\n",
    "    \n",
    "    # Select the first 10 documents\n",
    "    first_10_docs = all_docs[:10]\n",
    "    \n",
    "    # Initialize the TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(first_10_docs)\n",
    "    \n",
    "    # Compute the cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "# Example usage\n",
    "similarity_matrix = compute_similarity_for_first_10(documents)\n",
    "\n",
    "# Print the similarity matrix for the first 10 documents\n",
    "print(\"Similarity Matrix (First 10 Documents):\")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "t31ngfZTJimS",
   "metadata": {
    "id": "t31ngfZTJimS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrix (10 Random Documents):\n",
      "[[1.         0.42483855 0.73037102 0.62487471 0.57398258 0.51691625\n",
      "  0.53705175 0.59572903 0.61962474 0.57051859]\n",
      " [0.42483855 1.         0.48269049 0.47477452 0.45592912 0.37241368\n",
      "  0.44611859 0.44124871 0.49191732 0.44405895]\n",
      " [0.73037102 0.48269049 1.         0.69906811 0.68570154 0.60061654\n",
      "  0.6329849  0.66292598 0.74824945 0.65389716]\n",
      " [0.62487471 0.47477452 0.69906811 1.         0.69333676 0.77323196\n",
      "  0.63126641 0.68075239 0.72568615 0.68511696]\n",
      " [0.57398258 0.45592912 0.68570154 0.69333676 1.         0.60909226\n",
      "  0.59777255 0.63043674 0.69774946 0.61084449]\n",
      " [0.51691625 0.37241368 0.60061654 0.77323196 0.60909226 1.\n",
      "  0.50761389 0.58856072 0.57288325 0.55526598]\n",
      " [0.53705175 0.44611859 0.6329849  0.63126641 0.59777255 0.50761389\n",
      "  1.         0.58317117 0.6905728  0.56912925]\n",
      " [0.59572903 0.44124871 0.66292598 0.68075239 0.63043674 0.58856072\n",
      "  0.58317117 1.         0.67149559 0.60640506]\n",
      " [0.61962474 0.49191732 0.74824945 0.72568615 0.69774946 0.57288325\n",
      "  0.6905728  0.67149559 1.         0.66370146]\n",
      " [0.57051859 0.44405895 0.65389716 0.68511696 0.61084449 0.55526598\n",
      "  0.56912925 0.60640506 0.66370146 1.        ]]\n",
      "Indices of Random Documents: [133, 520, 536, 68, 228, 16, 279, 248, 144, 401]\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity scores for 10 random documents\n",
    "\n",
    "import random\n",
    "\n",
    "# Function to compute similarity scores for 10 random documents\n",
    "def compute_similarity_for_random_10(documents):\n",
    "    # Combine all document contents into a list\n",
    "    all_docs = []\n",
    "    for folder, docs in documents.items():\n",
    "        for doc in docs:\n",
    "            all_docs.append(doc[\"page_content\"])\n",
    "    \n",
    "    # Select 10 random documents\n",
    "    random_indices = random.sample(range(len(all_docs)), 10)\n",
    "    random_docs = [all_docs[i] for i in random_indices]\n",
    "    \n",
    "    # Initialize the TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(random_docs)\n",
    "    \n",
    "    # Compute the cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    return similarity_matrix, random_indices\n",
    "\n",
    "# Example usage\n",
    "similarity_matrix, random_indices = compute_similarity_for_random_10(documents)\n",
    "\n",
    "# Print the similarity matrix for the 10 random documents\n",
    "print(\"Similarity Matrix (10 Random Documents):\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "# Print the indices of the selected random documents\n",
    "print(\"Indices of Random Documents:\", random_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbfe6c1",
   "metadata": {},
   "source": [
    "### Observation: It seems that documents that are far from each other has lower similarity score. \n",
    "### The first 10 documents has averagely higher similarity score due to their nearer positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd0f53",
   "metadata": {
    "id": "3cfd0f53"
   },
   "source": [
    "### **1.4 Document Creation and Chunking** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pCw3NzcE3waS",
   "metadata": {
    "id": "pCw3NzcE3waS"
   },
   "source": [
    "#### **1.4.1** <font color=red> [5 marks] </font>\n",
    "Perform appropriate steps to split the text into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "TjZ6yf9r2p1F",
   "metadata": {
    "id": "TjZ6yf9r2p1F"
   },
   "outputs": [],
   "source": [
    "def generate_chunks(documents, chunk_size=500, chunk_overlap=50):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    for folder, docs in documents.items():\n",
    "        for doc in docs:\n",
    "            doc_chunks = text_splitter.split_text(doc[\"page_content\"])\n",
    "            for chunk in doc_chunks:\n",
    "                chunks.append({\n",
    "                    \"content\": chunk,\n",
    "                    \"metadata\": {\n",
    "                        \"source\": folder,\n",
    "                        \"file_name\": doc[\"metadata\"].get(\"file_name\", \"unknown\")  # Access metadata as a key\n",
    "                    }\n",
    "                })\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d9c6ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'confidentiality and nondisclosure agreement this confidentiality and nondisclosure agreement this agreement is dated _______ ___ 2018 the effective date and is between qep energy company owner a delaware corporation and _____________________ the receiving company a ______ ______________ owner and the receiving company are sometimes referred to herein individually as a party and collectively as the parties r e c i t a l s whereas owner has in its possession the confidential information as', 'metadata': {'source': 'contractnli', 'file_name': 'QEP-Williston-Form-of-Confidentiality-Agreement-BMO.txt'}}, {'content': 'in its possession the confidential information as hereinafter defined relating to owners and certain of its affiliates assets and properties located in the williston basin in dunn mckenzie mclean mercer and mountrail counties north dakota collectively the properties whereas in order for the receiving company to determine its interest in entering into a transaction with owner andor certain of its affiliates regarding the potential sale transfer or other disposition of all or any portion of the', 'metadata': {'source': 'contractnli', 'file_name': 'QEP-Williston-Form-of-Confidentiality-Agreement-BMO.txt'}}, {'content': 'or other disposition of all or any portion of the properties the transaction owner is willing to disclose on a nonexclusive basis certain confidential information to the receiving company for the sole purpose of the receiving companys review and evaluation of the transaction provided that the receiving company agrees to and accepts the terms and provisions of this agreement and whereas the receiving company is willing to agree to keep the confidential information confidential and to use such', 'metadata': {'source': 'contractnli', 'file_name': 'QEP-Williston-Form-of-Confidentiality-Agreement-BMO.txt'}}, {'content': 'information confidential and to use such information only in accordance with the terms of this agreement now therefore for and in consideration of the mutual covenants and agreements contained herein and other good and valuable consideration the receipt of which is hereby acknowledged the parties agree as follows a g r e e m e n t 1 nondisclosure of confidential information a the receiving company x shall not use and shall cause each of its representatives as hereinafter defined not to use any', 'metadata': {'source': 'contractnli', 'file_name': 'QEP-Williston-Form-of-Confidentiality-Agreement-BMO.txt'}}, {'content': 'as hereinafter defined not to use any confidential information including any portion thereof for any purpose other than in connection with the receiving companys evaluation of the potential transaction and y shall and shall cause each of its representatives to keep strictly confidential at all times and not disclose any confidential information except as expressly permitted hereunder the confidential information may be disclosed by the receiving company to any of the receiving companys', 'metadata': {'source': 'contractnli', 'file_name': 'QEP-Williston-Form-of-Confidentiality-Agreement-BMO.txt'}}]\n"
     ]
    }
   ],
   "source": [
    "chunks = generate_chunks(documents)\n",
    "print(chunks[:5])  # Print the first 5 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LeAeTqpZ-DYw",
   "metadata": {
    "id": "LeAeTqpZ-DYw"
   },
   "source": [
    "## **2. Vector Database and RAG Chain Creation** <font color=red> [15 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YoH_Ac6K6aQZ",
   "metadata": {
    "id": "YoH_Ac6K6aQZ"
   },
   "source": [
    "### **2.1 Vector Embedding and Vector Database Creation** <font color=red> [7 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bBfj5ycC59lU",
   "metadata": {
    "id": "bBfj5ycC59lU"
   },
   "source": [
    "#### **2.1.1** <font color=red> [2 marks] </font>\n",
    "Initialise an embedding function for loading the embeddings into the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v-QeR5N_7jiw",
   "metadata": {
    "id": "v-QeR5N_7jiw"
   },
   "source": [
    "Initialise a function to transform the text to vectors using OPENAI Embeddings module. You can also use this function to transform during vector DB creation itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3Jaq3HEhpxN",
   "metadata": {
    "id": "b3Jaq3HEhpxN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key fetched successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Fetch your OpenAI API Key from environment variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai_api_key:\n",
    "    print(\"OpenAI API Key fetched successfully.\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not found. Please set it as an environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "purQgINbhpxO",
   "metadata": {
    "id": "purQgINbhpxO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/h_gm4mw10s9f1vh17gj244n00000gn/T/ipykernel_45564/2918615772.py:16: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Embeddings initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialise an embedding function\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "def get_openai_embeddings(api_key):\n",
    "    \"\"\"\n",
    "    Initialize the OpenAI Embeddings module with the provided API key.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Your OpenAI API key.\n",
    "\n",
    "    Returns:\n",
    "        OpenAIEmbeddings: An instance of the OpenAIEmbeddings class.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "        print(\"OpenAI Embeddings initialized successfully.\")\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing OpenAI Embeddings: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "import os\n",
    "\n",
    "# Fetch the OpenAI API key from environment variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai_api_key:\n",
    "    embeddings = get_openai_embeddings(openai_api_key)\n",
    "else:\n",
    "    print(\"OpenAI API Key not found. Please set it as an environment variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WTkTIerj5-KI",
   "metadata": {
    "id": "WTkTIerj5-KI"
   },
   "source": [
    "#### **2.1.2** <font color=red> [5 marks] </font>\n",
    "Load the embeddings to a vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o6rEbd7477R8",
   "metadata": {
    "id": "o6rEbd7477R8"
   },
   "source": [
    "Create a directory for vector database and enter embedding data to the vector DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "IaqfjQJf2v8Y",
   "metadata": {
    "id": "IaqfjQJf2v8Y"
   },
   "outputs": [],
   "source": [
    "def load_embeddings_to_vector_db(chunks, api_key, persist_directory=\"vector_db\"):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "    try:\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "\n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=[chunk[\"content\"] for chunk in chunks],  # Access \"content\" key\n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_directory,\n",
    "            metadatas=[chunk[\"metadata\"] for chunk in chunks]  # Access \"metadata\" key\n",
    "        )\n",
    "\n",
    "        vector_db.persist()\n",
    "        print(f\"Vector database created and persisted at: {persist_directory}\")\n",
    "        return vector_db\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embeddings to vector database: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba5fe092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading embeddings to vector database: 'str' object has no attribute 'page_content'\n"
     ]
    }
   ],
   "source": [
    "vector_db = load_embeddings_to_vector_db(chunks, openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86cfa2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'confidentiality and nondisclosure agreement this confidentiality and nondisclosure agreement this agreement is dated _______ ___ 2018 the effective date and is between qep energy company owner a delaware corporation and _____________________ the receiving company a ______ ______________ owner and the receiving company are sometimes referred to herein individually as a party and collectively as the parties r e c i t a l s whereas owner has in its possession the confidential information as', 'metadata': {'source': 'contractnli', 'file_name': 'QEP-Williston-Form-of-Confidentiality-Agreement-BMO.txt'}}, {'content': 'in its possession the confidential information as hereinafter defined relating to owners and certain of its affiliates assets and properties located in the williston basin in dunn mckenzie mclean mercer and mountrail counties north dakota collectively the properties whereas in order for the receiving company to determine its interest in entering into a transaction with owner andor certain of its affiliates regarding the potential sale transfer or other disposition of all or any portion of the', 'metadata': {'source': 'contractnli', 'file_name': 'QEP-Williston-Form-of-Confidentiality-Agreement-BMO.txt'}}, {'content': 'or other disposition of all or any portion of the properties the transaction owner is willing to disclose on a nonexclusive basis certain confidential information to the receiving company for the sole purpose of the receiving companys review and evaluation of the transaction provided that the receiving company agrees to and accepts the terms and provisions of this agreement and whereas the receiving company is willing to agree to keep the confidential information confidential and to use such', 'metadata': {'source': 'contractnli', 'file_name': 'QEP-Williston-Form-of-Confidentiality-Agreement-BMO.txt'}}, {'content': 'information confidential and to use such information only in accordance with the terms of this agreement now therefore for and in consideration of the mutual covenants and agreements contained herein and other good and valuable consideration the receipt of which is hereby acknowledged the parties agree as follows a g r e e m e n t 1 nondisclosure of confidential information a the receiving company x shall not use and shall cause each of its representatives as hereinafter defined not to use any', 'metadata': {'source': 'contractnli', 'file_name': 'QEP-Williston-Form-of-Confidentiality-Agreement-BMO.txt'}}, {'content': 'as hereinafter defined not to use any confidential information including any portion thereof for any purpose other than in connection with the receiving companys evaluation of the potential transaction and y shall and shall cause each of its representatives to keep strictly confidential at all times and not disclose any confidential information except as expressly permitted hereunder the confidential information may be disclosed by the receiving company to any of the receiving companys', 'metadata': {'source': 'contractnli', 'file_name': 'QEP-Williston-Form-of-Confidentiality-Agreement-BMO.txt'}}]\n"
     ]
    }
   ],
   "source": [
    "print(chunks[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978619ac",
   "metadata": {
    "id": "978619ac"
   },
   "source": [
    "### **2.2 Create RAG Chain** <font color=red> [8 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rczna1Xy_1bq",
   "metadata": {
    "id": "Rczna1Xy_1bq"
   },
   "source": [
    "#### **2.2.1** <font color=red> [5 marks] </font>\n",
    "Create a RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sEzxYN93Ygju",
   "metadata": {
    "id": "sEzxYN93Ygju"
   },
   "outputs": [],
   "source": [
    "# Create a RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6PkgzeTIElfy",
   "metadata": {
    "id": "6PkgzeTIElfy"
   },
   "source": [
    "#### **2.2.2** <font color=red> [3 marks] </font>\n",
    "Create a function to generate answer for asked questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W8AMtr94FZxR",
   "metadata": {
    "id": "W8AMtr94FZxR"
   },
   "source": [
    "Use the RAG chain to generate answer for a question and provide source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9TQdz5uFzlr",
   "metadata": {
    "id": "b9TQdz5uFzlr"
   },
   "outputs": [],
   "source": [
    "# Create a function for question answering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pSgcP19iYgjv",
   "metadata": {
    "id": "pSgcP19iYgjv"
   },
   "outputs": [],
   "source": [
    "# Example question\n",
    "# question =\"Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document indicate that the Agreement does not grant the Receiving Party any rights to the Confidential Information?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fMmX8OrcN05D",
   "metadata": {
    "id": "fMmX8OrcN05D"
   },
   "source": [
    "## **3. RAG Evaluation** <font color=red> [10 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddfed9",
   "metadata": {
    "id": "b1ddfed9"
   },
   "source": [
    "### **3.1 Evaluation and Inference** <font color=red> [10 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z9xy_GduS9Yk",
   "metadata": {
    "id": "Z9xy_GduS9Yk"
   },
   "source": [
    "#### **3.1.1** <font color=red> [2 marks] </font>\n",
    "Extract all the questions and all the answers/ground truths from the benchmark files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V397RqkRfjSP",
   "metadata": {
    "id": "V397RqkRfjSP"
   },
   "source": [
    "Create a questions set and an answers set containing all the questions and answers from the benchmark files to run evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bF_KZXb1c-G5",
   "metadata": {
    "id": "bF_KZXb1c-G5"
   },
   "outputs": [],
   "source": [
    "# Create a question set by taking all the questions from the benchmark data\n",
    "# Also create a ground truth/answer set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81VscxuGTHhC",
   "metadata": {
    "id": "81VscxuGTHhC"
   },
   "source": [
    "#### **3.1.2** <font color=red> [5 marks] </font>\n",
    "Create a function to evaluate the generated answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qIPUg1dKTPGb",
   "metadata": {
    "id": "qIPUg1dKTPGb"
   },
   "source": [
    "Evaluate the responses on *Rouge*, *Ragas* and *Bleu* scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "RuoBJS5_PKmX",
   "metadata": {
    "id": "RuoBJS5_PKmX"
   },
   "outputs": [],
   "source": [
    "# Function to evaluate the RAG pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Omeb5vFSTbS0",
   "metadata": {
    "id": "Omeb5vFSTbS0"
   },
   "source": [
    "#### **3.1.3** <font color=red> [3 marks] </font>\n",
    "Draw inferences by evaluating answers to all questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ei2qIN71Tirg",
   "metadata": {
    "id": "ei2qIN71Tirg"
   },
   "source": [
    "To save time and computing power, you can just run the evaluation on first 100 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4f1f24a",
   "metadata": {
    "id": "f4f1f24a"
   },
   "outputs": [],
   "source": [
    "# Evaluate the RAG pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gonMO9wNE5dt",
   "metadata": {
    "id": "gonMO9wNE5dt"
   },
   "source": [
    "## **4. Conclusion** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ySHPR29rE5du",
   "metadata": {
    "id": "ySHPR29rE5du"
   },
   "source": [
    "### **4.1 Conclusions and insights** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KoVsmcV0E5du",
   "metadata": {
    "id": "KoVsmcV0E5du"
   },
   "source": [
    "#### **4.1.1** <font color=red> [5 marks] </font>\n",
    "Conclude with the results here. Include the insights gained about the data, model pipeline, the RAG process and the results obtained."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "LeAeTqpZ-DYw",
    "fMmX8OrcN05D",
    "gonMO9wNE5dt"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
